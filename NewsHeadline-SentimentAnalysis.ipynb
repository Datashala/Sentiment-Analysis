{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'https://raw.githubusercontent.com/krishnaik06/Stock-Sentiment-Analysis/master/Data.csv'\n",
    "# res = requests.get(url, allow_redirects=True)\n",
    "# with open('news_headlines.csv','wb') as file:\n",
    "#     file.write(res.content)\n",
    "news = pd.read_csv('news_headlines.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4101 entries, 0 to 4100\n",
      "Data columns (total 27 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    4101 non-null   object\n",
      " 1   Label   4101 non-null   int64 \n",
      " 2   Top1    4101 non-null   object\n",
      " 3   Top2    4101 non-null   object\n",
      " 4   Top3    4101 non-null   object\n",
      " 5   Top4    4101 non-null   object\n",
      " 6   Top5    4101 non-null   object\n",
      " 7   Top6    4101 non-null   object\n",
      " 8   Top7    4101 non-null   object\n",
      " 9   Top8    4101 non-null   object\n",
      " 10  Top9    4101 non-null   object\n",
      " 11  Top10   4101 non-null   object\n",
      " 12  Top11   4101 non-null   object\n",
      " 13  Top12   4101 non-null   object\n",
      " 14  Top13   4101 non-null   object\n",
      " 15  Top14   4101 non-null   object\n",
      " 16  Top15   4101 non-null   object\n",
      " 17  Top16   4101 non-null   object\n",
      " 18  Top17   4101 non-null   object\n",
      " 19  Top18   4101 non-null   object\n",
      " 20  Top19   4101 non-null   object\n",
      " 21  Top20   4101 non-null   object\n",
      " 22  Top21   4101 non-null   object\n",
      " 23  Top22   4101 non-null   object\n",
      " 24  Top23   4100 non-null   object\n",
      " 25  Top24   4098 non-null   object\n",
      " 26  Top25   4098 non-null   object\n",
      "dtypes: int64(1), object(26)\n",
      "memory usage: 865.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Knowning the data\n",
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of numerical variables in the dataset\n",
    "news.select_dtypes(exclude='object').columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of string variables in the dataset\n",
    "news.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the data is balanced or imbalanced\n",
    "(news.Label.value_counts()/len(news))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 52% of one category and  47% of another. A Balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Null values\n",
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding rows with Null values\n",
    "news[news.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling NA with something that does not affect learning\n",
    "news.fillna(' ',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = news[news['Date'] < '2014']\n",
    "test = news[news['Date'] > '2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (3471, 27) \n",
      "Shape of test data: (630, 27) \n",
      "Percentage of data used for training: 84.64% \n",
      "Percentage of data used for testing: 15.36% \n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data: {} \".format(train.shape))\n",
    "print(\"Shape of test data: {} \".format(test.shape))\n",
    "print(\"Percentage of data used for training: {:0.2f}% \".format((train.shape[0]/len(news))*100))\n",
    "print(\"Percentage of data used for testing: {:0.2f}% \".format((test.shape[0]/len(news))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using this rule based spliting, we got 85:15 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can first tokenize using some NLTK tokenizer and then use vectoriser of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer \n",
    "     \n",
    "# Create a reference variable for Class WordPunctTokenizer \n",
    "tk = WordPunctTokenizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the 108-year-old first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "#Using our tokenizer.\n",
    "# corp=[tk.tokenize(i) for i in corpus]\n",
    "# def dummy(doc):\n",
    "#     return doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))#,lowercase=False,preprocessor=dummy,tokenizer=dummy)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train.iloc[:,2:]\n",
    "label=train.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations \n",
    "train_data.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the complete dataframe into lower case\n",
    "train_data=train_data.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Corpus\n",
    "headlines = []\n",
    "for row in range(0,len(train_data)):\n",
    "    headlines.append(' '.join(str(x) for x in train_data.iloc[row,0:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a  hindrance to operations   extracts from the leaked reports scorecard hughes  instant hit buoys blues jack gets his skates on at ice cold alex chaos as maracana builds up for united depleted leicester prevail as elliott spoils everton s party hungry spurs sense rich pickings gunners so wide of an easy target derby raise a glass to strupar s debut double southgate strikes  leeds pay the penalty hammers hand robson a youthful lesson saints party like it s      wear wolves have turned into lambs stump mike catches testy gough s taunt langer escapes to hit     flintoff injury piles on woe for england hunters threaten jospin with new battle of the somme kohl s successor drawn into scandal the difference between men and women sara denver  nurse turned solicitor diana s landmine crusade put tories in a panic yeltsin s resignation caught opposition flat footed russian roulette sold out recovering a title'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement BAG OF WORDS\n",
    "countvector=CountVectorizer(ngram_range=(2,2))\n",
    "traindataset=countvector.fit_transform(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=200)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement RandomForest Classifier\n",
    "randomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\n",
    "randomclassifier.fit(traindataset,train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test.iloc[:,2:]\n",
    "true_y=test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations \n",
    "test_data.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "#Convert the complete dataframe into lower case\n",
    "test_data=test_data.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_transform= []\n",
    "# for row in range(0,len(test.index)):\n",
    "#     test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "# test_dataset = countvector.transform(test_transform)\n",
    "# predictions = randomclassifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Predict for the Test Dataset\n",
    "# test_transform= []\n",
    "# for row in range(0,len(test_data.index)):\n",
    "#     test_transform.append(' '.join(str(x) for x in test_data.iloc[row,0:25]))\n",
    "test_dataset = countvector.transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = randomclassifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import library to check accuracy\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 42 257]\n",
      " [ 32 299]]\n",
      "0.5412698412698412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.14      0.23       299\n",
      "           1       0.54      0.90      0.67       331\n",
      "\n",
      "    accuracy                           0.54       630\n",
      "   macro avg       0.55      0.52      0.45       630\n",
      "weighted avg       0.55      0.54      0.46       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes.fit(traindataset,train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bayes.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26 273]\n",
      " [ 26 305]]\n",
      "0.5253968253968254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.09      0.15       299\n",
      "           1       0.53      0.92      0.67       331\n",
      "\n",
      "    accuracy                           0.53       630\n",
      "   macro avg       0.51      0.50      0.41       630\n",
      "weighted avg       0.51      0.53      0.42       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix=confusion_matrix(test['Label'],predictions)\n",
    "print(matrix)\n",
    "score=accuracy_score(test['Label'],predictions)\n",
    "print(score)\n",
    "report=classification_report(test['Label'],predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=int64),\n",
       " array([312140001,    139457,      4073,       388,       111,        27,\n",
       "                4,         5,         4], dtype=int64))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_dataset.toarray(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
